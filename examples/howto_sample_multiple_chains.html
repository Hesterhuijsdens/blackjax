
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Sample with multiple chains in parallel</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Use custom gradients" href="howto_custom_gradients.html" />
    <link rel="prev" title="Use with TFP models" href="howto_use_tfp.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/blackjax.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  PPL INTEGRATION
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="howto_use_aesara.html">
   Aesara
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="howto_use_numpyro.html">
   Numpyro
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="howto_use_oryx.html">
   Oryx
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="howto_use_pymc.html">
   PyMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="howto_use_tfp.html">
   Tensorflow-Probability
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  HOW TO
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Sample with multiple chains?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="howto_custom_gradients.html">
   Use custom gradients?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="howto_other_frameworks.html">
   Use non-JAX log-prob functions?
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  LEARN BY EXAMPLE
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../examples.html">
   Examples
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Introduction.html">
     A Quick Introduction to Blackjax
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="LogisticRegression.html">
     Bayesian Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="LogisticRegressionWithLatentGaussianSampler.html">
     Bayesian Logistic Regression With Latent Gaussian Sampler
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="TemperedSMC.html">
     Use Tempered SMC to Improve Exploration of MCMC Methods.
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="HierarchicalBNN.html">
     Hierarchical Bayesian Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="PeriodicOrbitalMCMC.html">
     Periodic Orbital MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="GP_EllipticalSliceSampler.html">
     Gaussian Regression with the Elliptical Slice Sampler
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="GP_Marginal.html">
     Bayesian Regression With Latent Gaussian Sampler
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="SGMCMC.html">
     MNIST Digit Recognition With a 3-Layer Perceptron
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="change_of_variable_hmc.html">
     Change of Variable in HMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Pathfinder.html">
     Pathfinder
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="RegimeSwitchingModel.html">
     Regime switching Hidden Markov model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="SparseLogisticRegression.html">
     Sparse logistic regression
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  API
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../mcmc.html">
   MCMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../sgmcmc.html">
   Stochastic gradient MCMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../smc.html">
   Sequential Monte Carlo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../vi.html">
   Variational Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../adaptation.html">
   Adaptation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../diagnostics.html">
   Diagnostics
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/blackjax-devs/blackjax"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vectorization-vs-parallelization">
   Vectorization vs parallelization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nuts-in-parallel">
   NUTS in parallel
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-jax-vmap">
     Using
     <code class="docutils literal notranslate">
      <span class="pre">
       jax.vmap
      </span>
     </code>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-jax-pmap">
     Using
     <code class="docutils literal notranslate">
      <span class="pre">
       jax.pmap
      </span>
     </code>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-note-on-using-jax-pmap-on-cpu">
       A note on using
       <code class="docutils literal notranslate">
        <span class="pre">
         jax.pmap
        </span>
       </code>
       on CPU
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#choosing-the-number-of-devices">
         Choosing the number of devices
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#back-to-our-example">
     Back to our example
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conclusions">
     Conclusions
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Sample with multiple chains in parallel</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vectorization-vs-parallelization">
   Vectorization vs parallelization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nuts-in-parallel">
   NUTS in parallel
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-jax-vmap">
     Using
     <code class="docutils literal notranslate">
      <span class="pre">
       jax.vmap
      </span>
     </code>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-jax-pmap">
     Using
     <code class="docutils literal notranslate">
      <span class="pre">
       jax.pmap
      </span>
     </code>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-note-on-using-jax-pmap-on-cpu">
       A note on using
       <code class="docutils literal notranslate">
        <span class="pre">
         jax.pmap
        </span>
       </code>
       on CPU
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#choosing-the-number-of-devices">
         Choosing the number of devices
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#back-to-our-example">
     Back to our example
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conclusions">
     Conclusions
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="sample-with-multiple-chains-in-parallel">
<h1>Sample with multiple chains in parallel<a class="headerlink" href="#sample-with-multiple-chains-in-parallel" title="Permalink to this headline">#</a></h1>
<p>Sampling with a few chains has become ubiquitous in modern probabilistic programming because it allows to compute better convergence diagnostics such as <span class="math notranslate nohighlight">\(\hat{R}\)</span>. More recently a new trend has emerged where researchers try to sample with thousands of chains for only a few steps. Whatever your use case is, Blackjax has you covered: thanks to JAX’s primitives you will be able to run multiple chains on CPU, GPU or TPU.</p>
<section id="vectorization-vs-parallelization">
<h2>Vectorization vs parallelization<a class="headerlink" href="#vectorization-vs-parallelization" title="Permalink to this headline">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">JAX</span></code> provides two distinct primitives to “run things in parallel”, and it is important to understand the difference to make the best use of Blackjax:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://jax.readthedocs.io/en/latest/jax.html?highlight=vmap#jax.vmap">jax.vmap</a> is used to SIMD vectorize <code class="docutils literal notranslate"><span class="pre">JAX</span></code> code. It is important to remember that vectorization happens at the <em>instruction level</em>, each CPU or GPU  instruction will the process the information from your different chains, <em>one intructions at a time</em>. This can have some unexpected consequences;</p></li>
<li><p><a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html#jax.pmap">jax.pmap</a> is a higher level abstraction, where processes are split across multiple devices: GPUs, TPUs, or CPU cores.</p></li>
</ul>
<p>For detailed walkthrough both primitives we invite your to read JAX’s tutorials on <a class="reference external" href="https://jax.readthedocs.io/en/latest/jax-101/03-vectorization.html">Automatic Vectorization</a>
and <a class="reference external" href="https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html">Parallel Evaluation</a>.</p>
</section>
<section id="nuts-in-parallel">
<h2>NUTS in parallel<a class="headerlink" href="#nuts-in-parallel" title="Permalink to this headline">#</a></h2>
<p>In the following we will sample from a linear regression with a NUTS sampler. This will illustrate the inherent limits with using <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code> when sampling with adaptative algorithms such as NUTS.</p>
<p>The model is</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">jax.scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>


<span class="n">loc</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span>
<span class="n">observed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1_000</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">logdensity_fn</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">log_scale</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">observed</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Univariate Normal&quot;&quot;&quot;</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_scale</span><span class="p">)</span>
    <span class="n">logpdf</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">observed</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">logpdf</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">logdensity</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">logdensity_fn</span><span class="p">(</span><span class="o">**</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">inference_loop</span><span class="p">(</span><span class="n">rng_key</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">initial_state</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>

    <span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
    <span class="k">def</span> <span class="nf">one_step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">rng_key</span><span class="p">):</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">(</span><span class="n">rng_key</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state</span><span class="p">,</span> <span class="n">state</span>

    <span class="n">keys</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng_key</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">one_step</span><span class="p">,</span> <span class="n">initial_state</span><span class="p">,</span> <span class="n">keys</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">states</span>
</pre></div>
</div>
</div>
</div>
<p>To make our demonstration more dramatic we will used a NUTS sampler with poorly chosen parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">blackjax</span>


<span class="n">inv_mass_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">])</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="mf">1e-3</span>

<span class="n">nuts</span> <span class="o">=</span> <span class="n">blackjax</span><span class="o">.</span><span class="n">nuts</span><span class="p">(</span><span class="n">logdensity</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">inv_mass_matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And finally, to put <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code> and <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code> on an equal foot we sample as many chains as the machine has CPU cores:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">multiprocessing</span>


<span class="n">rng_key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">num_chains</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
</pre></div>
</div>
</div>
</div>
<section id="using-jax-vmap">
<h3>Using <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code><a class="headerlink" href="#using-jax-vmap" title="Permalink to this headline">#</a></h3>
<p>Newcomers to JAX immediately recognize the benefits of using <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code>, and for a good reason: easily transforming any function into a universal function that will execute instructions in parallel is awesome!</p>
<p>Here we apply <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code> inside the <code class="docutils literal notranslate"><span class="pre">one_step</span></code> function and vectorize the transition kernel:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">inference_loop_multiple_chains</span><span class="p">(</span>
    <span class="n">rng_key</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">initial_state</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">num_chains</span>
<span class="p">):</span>

    <span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
    <span class="k">def</span> <span class="nf">one_step</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">rng_key</span><span class="p">):</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng_key</span><span class="p">,</span> <span class="n">num_chains</span><span class="p">)</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">kernel</span><span class="p">)(</span><span class="n">keys</span><span class="p">,</span> <span class="n">states</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">states</span><span class="p">,</span> <span class="n">states</span>

    <span class="n">keys</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng_key</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">one_step</span><span class="p">,</span> <span class="n">initial_state</span><span class="p">,</span> <span class="n">keys</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">states</span>
</pre></div>
</div>
</div>
</div>
<p>We now prepare the initial states using <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code> again, to vectorize the <code class="docutils literal notranslate"><span class="pre">init</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">initial_positions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_chains</span><span class="p">),</span> <span class="s2">&quot;log_scale&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_chains</span><span class="p">)}</span>
<span class="n">initial_states</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">nuts</span><span class="o">.</span><span class="n">init</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">))(</span><span class="n">initial_positions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And finally run the sampler</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">states</span> <span class="o">=</span> <span class="n">inference_loop_multiple_chains</span><span class="p">(</span>
    <span class="n">rng_key</span><span class="p">,</span> <span class="n">nuts</span><span class="o">.</span><span class="n">step</span><span class="p">,</span> <span class="n">initial_states</span><span class="p">,</span> <span class="mi">2_000</span><span class="p">,</span> <span class="n">num_chains</span>
<span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">states</span><span class="o">.</span><span class="n">position</span><span class="p">[</span><span class="s2">&quot;loc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We’ll let you judge of the correctness of the samples obtained (see the introduction notebook), but one thing should be obvious to you if you’ve samples with single chains with Blackjax before: <strong>it is slow!</strong></p>
<p>Remember when we said SIMD vectorization happens at the instruction level? At each step, the NUTS sampler can perform from 1 to 1024 integration steps, and the CPU (GPU) has to wait for all the chains to complete before moving on to the next chain. As a result, each step is as long as the slowest chain.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You may be thinking that instead of applying <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code> to <code class="docutils literal notranslate"><span class="pre">one_step</span></code> we could apply it to the <code class="docutils literal notranslate"><span class="pre">inference_loop_multiple_chains</span></code>, and the chains will run independently. Unfortunately, this is not how SIMD vectorization work although, granted, JAX’s user interface could led you to think otherwise.</p>
</div>
</section>
<section id="using-jax-pmap">
<h3>Using <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code><a class="headerlink" href="#using-jax-pmap" title="Permalink to this headline">#</a></h3>
<p>Now you may be thinking: we are limited by one chain if we synchronize at the step level, but things being random, chains that run truly in parallel should take in total roughly similar numbers of integration steps. So <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code> should help here. This is true, let’s prove it!</p>
<section id="a-note-on-using-jax-pmap-on-cpu">
<h4>A note on using <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code> on CPU<a class="headerlink" href="#a-note-on-using-jax-pmap-on-cpu" title="Permalink to this headline">#</a></h4>
<p>JAX will treat your CPU as a single device by default, regardless of the number of cores available.</p>
<p>Unfortunately, this means that using <code class="docutils literal notranslate"><span class="pre">pmap</span></code> is not possible out of the box – we’ll first need
to instruct JAX to split the CPU into multiple devices. See <a class="reference external" href="https://github.com/google/jax/issues/1408">this issue</a> for more discussion on this topic.</p>
<p>Currently, this can only be done via <code class="docutils literal notranslate"><span class="pre">XLA_FLAGS</span></code> environmental variable.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This variable has to be set before JAX or any library that imports it is imported</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">multiprocessing</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;XLA_FLAGS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;--xla_force_host_platform_device_count=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">multiprocessing</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We advise you to confirm that JAX has successfuly recognized our CPU as multiple devices with the following command before moving forward:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>

<span class="nb">len</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2
</pre></div>
</div>
</div>
</div>
<section id="choosing-the-number-of-devices">
<h5>Choosing the number of devices<a class="headerlink" href="#choosing-the-number-of-devices" title="Permalink to this headline">#</a></h5>
<p><code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code> has one more limitation: it is not able to parallelize the execution when you ask it to perform more computations than there are available deviced. The following code snippet asks <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code> perform 1024 operations in parallel:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>

<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1024</span><span class="p">)</span>
    <span class="n">parallel_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">pmap</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">parallel_fn</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>compiling computation that requires 1024 logical devices, but only 2 XLA devices are available (num_replicas=1024, num_partitions=1)
</pre></div>
</div>
</div>
</div>
<p>This means that you will only be able to run as many MCMC chains as you have CPU cores. See this <a class="reference external" href="https://github.com/google/jax/discussions/4198">question</a> for a more detailed discussion,
and a workaround involving nesting <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code> and <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code> calls.</p>
<p>Another option (we advise against) is to set the device count to a number larger than the core count, e.g. <code class="docutils literal notranslate"><span class="pre">200</span></code>, but it’s <a class="reference external" href="https://github.com/google/jax/issues/1408#issuecomment-536158048">unclear what side effects it might have</a>.</p>
</section>
</section>
</section>
<section id="back-to-our-example">
<h3>Back to our example<a class="headerlink" href="#back-to-our-example" title="Permalink to this headline">#</a></h3>
<p>In case of <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code>, we apply the transformation directly to the original <code class="docutils literal notranslate"><span class="pre">inference_loop</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inference_loop_multiple_chains</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">pmap</span><span class="p">(</span><span class="n">inference_loop</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">static_broadcasted_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We could have done that in the <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code> example (and it wouldn’t have helped), but we prefered to highlight in the code the fact that vectorization happens at the instruction level.</p>
</div>
<p>We are now ready to sample:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keys</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng_key</span><span class="p">,</span> <span class="n">num_chains</span><span class="p">)</span>

<span class="n">pmap_states</span> <span class="o">=</span> <span class="n">inference_loop_multiple_chains</span><span class="p">(</span>
    <span class="n">keys</span><span class="p">,</span> <span class="n">nuts</span><span class="o">.</span><span class="n">step</span><span class="p">,</span> <span class="n">initial_states</span><span class="p">,</span> <span class="mi">2_000</span>
<span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">pmap_states</span><span class="o">.</span><span class="n">position</span><span class="p">[</span><span class="s2">&quot;loc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Wow, this was much faster, our intuition was correct! Note that the samples are transposed compared to the ones obtained with <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code>.</p>
</section>
<section id="conclusions">
<h3>Conclusions<a class="headerlink" href="#conclusions" title="Permalink to this headline">#</a></h3>
<p>In this example the different between <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code> and <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code> is dramatic: it takes several minutes to <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code> and a few seconds for <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code> to sample the same number of chains. This is expected for NUTS, and other adaptive algorithms: each chain runs a different number of internal steps for each sample generated and we need to wait for the slowest chain.</p>
<p>We saw one possible solutions for those who just want a few chains to run diagnostics: parallelize using <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code>. For the thousands of chains we mentionned earlier you will need something different: either distribute on several machine (expensive), or design new algorithms altogether. HMC, for instance, runs the same number of integration steps on every chain and thus doesn’t exhibit the same synchronization problem. That’s the idea behind algorithms like ChEEs and MEADS! This is a very active area of research, and now you understand why.</p>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="howto_use_tfp.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Use with TFP models</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="howto_custom_gradients.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Use custom gradients</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Blackjax developers<br/>
  
      &copy; Copyright 2023, The Blackjax developers.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>